services:

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "8082:8082"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'kafka:29092'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      CLUSTER_ID: '4L6g3nShT-eMCtK--X86sw'
    healthcheck: {test: nc -z localhost 9092, interval: 1s, start_period: 120s}
    networks:
      - kafka-net

  connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    container_name: connect
    hostname: connect
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "__connect-configs"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: "__connect-offsets"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: "__connect-status"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    volumes:
      - ./plugins:/usr/share/confluent-hub-components
    depends_on:
      kafka: {condition: service_healthy}
      schema-registry: {condition: service_started}
    healthcheck: {test: curl -f http://localhost:8083, interval: 10s, timeout: 5s, retries: 10}
    networks:
      - kafka-net

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: schema-registry
    hostname: schema-registry
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
    depends_on:
      kafka: {condition: service_healthy}
    healthcheck:
      test: curl -f http://schema-registry:8081/subjects || exit 1
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - kafka-net

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:v3.1.0
    container_name: redpanda-console
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml && /app/console -config.filepath /tmp/config.yml'
    environment:
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["kafka:29092"]
        schemaRegistry:
          enabled: true
          urls: ["http://schema-registry:8081"]
        kafkaConnect:
          enabled: true
          clusters:
            - name: cdr_sink
              url: http://connect:8083
    ports:
      - 9001:8080
    depends_on:
      kafka: {condition: service_healthy}
      schema-registry: {condition: service_healthy}
      connect: {condition: service_healthy}
    networks:
      - kafka-net

  # spark-master:
  #   image: bitnami/spark:3.1
  #   container_name: spark-master
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #   volumes:
  #     - ./jars:/home/spark/jars
  #   ports:
  #     - "7077:7077"
  #     - "8080:8080"
  #   networks:
  #     - kafka-net

  # spark-worker-1:
  #   image: bitnami/spark:3.1
  #   container_name: spark-worker-1
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #   volumes:
  #     - ./jars:/home/spark/jars
  #   ports:
  #     - "8084:8081"
  #   networks:
  #     - kafka-net

  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: mydb
    ports:
      - "6400:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./initdb:/docker-entrypoint-initdb.d
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
  
  mongodb:
    image: 'mongo:latest'
    environment:
      - 'MONGO_INITDB_DATABASE=mydb'
      - 'MONGO_INITDB_ROOT_PASSWORD=password'
      - 'MONGO_INITDB_ROOT_USERNAME=root'
    ports:
      - '27017:27017'
    volumes:
        - 'mongodb_data:/data/db'
    networks:
      - kafka-net
    healthcheck:
      test: >
        mongosh --username root --password password --authenticationDatabase admin --eval "db.adminCommand('ping')"
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s


  pinot-zookeeper:
    image: zookeeper:3.9.2
    container_name: "pinot-zookeeper"
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
  pinot-controller:
    image: apachepinot/pinot:1.2.0
    command: "StartController -zkAddress pinot-zookeeper:2181"
    container_name: "pinot-controller"
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms1G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-controller.log"
    depends_on:
      pinot-zookeeper:
        condition: service_healthy
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

  pinot-broker:
    image: apachepinot/pinot:1.2.0
    command: "StartBroker -zkAddress pinot-zookeeper:2181"
    container_name: "pinot-broker"
    restart: unless-stopped
    ports:
      - "8099:8099"
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-broker.log"
    depends_on:
      pinot-controller:
        condition: service_healthy
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8099/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

  pinot-server:
    image: apachepinot/pinot:1.2.0
    command: "StartServer -zkAddress pinot-zookeeper:2181"
    container_name: "pinot-server"
    restart: unless-stopped
    ports:
      - "8098:8098"
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx16G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-server.log"
    depends_on:
      pinot-broker:
        condition: service_healthy
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8097/health/readiness || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

  superset:
    image: apachesuperset.docker.scarf.sh/apache/superset:latest-dev
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=supersecretkey
      - DATABASE_URL=postgresql+psycopg2://superset:superset@metadata-db:5432/superset
    depends_on:
      metadata-db: {condition: service_healthy}

    command: >
      /bin/bash -c "
        pip install --no-cache-dir pinotdb &&
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin &&
        superset init &&
        superset import-dashboards --path /home/superset_bootstrap/dashboard1.zip -u admin &&
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debug
      "
    networks:
      - kafka-net
    volumes:
      - ./superset-bootstrap:/home/superset_bootstrap
  metadata-db:
    image: postgres
    container_name: metadata-db
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    volumes:
      - superset_db_data:/var/lib/postgresql/data
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  streaming-mediation-voice:
    image: oussamaend34/streaming-mediation
    network_mode: host
    scale: 1
    environment:
      APPLICATION_ID_CONFIG: "streaming-mediation-voice"
      BOOTSTRAP_SERVERS_CONFIG: "localhost:9092"
      SCHEMA_REGISTRY_URL_CONFIG: "http://localhost:8081"
      INPUT_TOPIC: "cdr.voice"
      OK_OUTPUT_TOPIC: "cdr.ok"
      ERROR_OUTPUT_TOPIC: "cdr.error"
      CDR_TYPE: "voice"
    restart: unless-stopped
    depends_on:
      kafka: {condition: service_healthy}
      connect: {condition: service_healthy}
      schema-registry: {condition: service_healthy}
      mongodb: {condition: service_healthy}
      postgres: {condition: service_healthy}
      prepare-env: {condition: service_completed_successfully}



  streaming-mediation-data:
    image: oussamaend34/streaming-mediation
    network_mode: host
    scale: 1
    environment:
      APPLICATION_ID_CONFIG: "streaming-mediation-data"
      BOOTSTRAP_SERVERS_CONFIG: "localhost:9092"
      SCHEMA_REGISTRY_URL_CONFIG: "http://localhost:8081"
      INPUT_TOPIC: "cdr.data"
      OK_OUTPUT_TOPIC: "cdr.ok"
      ERROR_OUTPUT_TOPIC: "cdr.error"
      CDR_TYPE: "data"
    depends_on:
      kafka: {condition: service_healthy}
      connect: {condition: service_healthy}
      schema-registry: {condition: service_healthy}
      mongodb: {condition: service_healthy}
      postgres: {condition: service_healthy}
      prepare-env: {condition: service_completed_successfully}

  streaming-mediation-sms:
    image: oussamaend34/streaming-mediation
    network_mode: host
    scale: 1
    environment:
      APPLICATION_ID_CONFIG: "streaming-mediation-sms"
      BOOTSTRAP_SERVERS_CONFIG: "localhost:9092"
      SCHEMA_REGISTRY_URL_CONFIG: "http://localhost:8081"
      INPUT_TOPIC: "cdr.sms"
      OK_OUTPUT_TOPIC: "cdr.ok"
      ERROR_OUTPUT_TOPIC: "cdr.error"
      CDR_TYPE: "sms"
    depends_on:
      kafka: {condition: service_healthy}
      connect: {condition: service_healthy}
      schema-registry: {condition: service_healthy}
      mongodb: {condition: service_healthy}
      postgres: {condition: service_healthy}
      prepare-env: {condition: service_completed_successfully}
    
  prepare-env:
    build:
      context: ../6-prepare-environment
      dockerfile: Dockerfile
    container_name: prepare-env
    network_mode: host
    depends_on:
      kafka: {condition: service_healthy}
      connect: {condition: service_healthy}
      schema-registry: {condition: service_healthy}
      mongodb: {condition: service_healthy}
      postgres: {condition: service_healthy}
      pinot-broker: {condition: service_healthy}
      pinot-controller: {condition: service_healthy}
      pinot-server: {condition: service_healthy}
      pinot-zookeeper: {condition: service_healthy}

  data-generator:
    build:
      context: ../1-synthetic_cdr_generator
      dockerfile: Dockerfile
    container_name: data-generator
    network_mode: host
    depends_on:
      prepare-env: {condition: service_completed_successfully}


volumes:
  postgres-data:
  mongodb_data:
  superset_db_data:


networks:
  kafka-net:
    driver: bridge
